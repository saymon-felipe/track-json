import tiktoken
import re
import time
import logging
from openai import OpenAI
from datetime import datetime

# Configura√ß√£o do cliente OpenAI
client = OpenAI(
    api_key=""
)

# Par√¢metros de token e chunk
MAX_TOKENS = 7000  # Limite de tokens do modelo
CHUNK_SIZE = 50    # Tente come√ßar com um valor menor e ajusta conforme necess√°rio

# Configura√ß√£o do log
def setup_logging():
    # Criando um nome √∫nico para o arquivo de log com base na data e hora
    log_filename = f"log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log"
    
    # Criando o arquivo de log
    logging.basicConfig(
        filename=log_filename,
        level=logging.INFO,
        format='%(asctime)s - %(message)s',
    )

    # Adicionando tamb√©m ao console
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
    logging.getLogger().addHandler(console_handler)

    logging.info("In√≠cio da execu√ß√£o do script")

# Fun√ß√£o para contar os tokens em um texto
def count_tokens(text):
    """Conta o n√∫mero de tokens em um texto usando tiktoken"""
    encoding = tiktoken.get_encoding("cl100k_base")
    tokens = len(encoding.encode(text))
    logging.info(f"Contagem de tokens: {tokens} para o texto '{text[:30]}...'")
    print(f"Contagem de tokens: {tokens} para o texto '{text[:30]}...'")  # Exibindo no console
    return tokens

# Fun√ß√£o para dividir o texto em peda√ßos menores
def split_into_chunks(filenames):
    chunks = []
    current_chunk = []
    current_chunk_tokens = 0

    logging.info(f"Iniciando a divis√£o dos arquivos em chunks...")
    print("Iniciando a divis√£o dos arquivos em chunks...")  # Exibindo no console
    
    for filename in filenames:
        chunk_text = f"{filename}"
        chunk_tokens = count_tokens(chunk_text)

        # Se adicionar o item ultrapassar o limite de tokens, cria um novo chunk
        if current_chunk_tokens + chunk_tokens > MAX_TOKENS:
            chunks.append(current_chunk)
            logging.info(f"Chunk finalizado com {len(current_chunk)} m√∫sicas.")
            print(f"Chunk finalizado com {len(current_chunk)} m√∫sicas.")  # Exibindo no console
            current_chunk = [filename]
            current_chunk_tokens = chunk_tokens
        else:
            current_chunk.append(filename)
            current_chunk_tokens += chunk_tokens

    if current_chunk:  # Adiciona o √∫ltimo chunk
        chunks.append(current_chunk)
        logging.info(f"√öltimo chunk adicionado com {len(current_chunk)} m√∫sicas.")
        print(f"√öltimo chunk adicionado com {len(current_chunk)} m√∫sicas.")  # Exibindo no console
    
    logging.info(f"Divis√£o completa: {len(chunks)} chunks criados.")
    print(f"Divis√£o completa: {len(chunks)} chunks criados.")  # Exibindo no console
    return chunks

# Fun√ß√£o para formatar os nomes das m√∫sicas
def format_song_name(filenames):
    setup_logging()
    chunks = split_into_chunks(filenames)
    formatted_names = []

    processed_files = 0
    
    for i, chunk in enumerate(chunks):
        chunk_text = ", ".join(chunk)
        prompt = f"""
        Os nomes de arquivos que eu vou mandar parecem ser de m√∫sicas. 
        Identifique o nome de cada m√∫sica e o seu respectivo artista com base no nome. 
        Retorne a lista no formato: 1: Artista - M√∫sica, 2: Artista - M√∫sica, etc".
        
        {chunk_text}
        """
        
        logging.info(f"Enviando o prompt para a OpenAI (Chunk {i + 1}/{len(chunks)}): {chunk_text[:50]}...")
        print(f"Enviando o prompt para a OpenAI (Chunk {i + 1}/{len(chunks)}): {chunk_text[:50]}...")  # Exibindo no console

        try:
            # Solicita√ß√£o √† OpenAI
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )

            formatted_list = response.choices[0].message.content.strip()

            # Verificando se a resposta tem o formato desejado
            if formatted_list:
                logging.info(f"Resposta recebida do GPT-4 para o chunk {i + 1}")
                print(f"Resposta recebida do GPT-4 para o chunk {i + 1}")  # Exibindo no console
                # Extraindo o nome da m√∫sica formatado
                formatted_names.extend(re.findall(r'\d+: (.+? - .+?)(?=,|$)', formatted_list))
                processed_files += len(formatted_names)

                print(f"üîÑ Processadas {processed_files} m√∫sicas de {len(filenames)} at√© agora.")
                logging.info(f"üîÑ Processadas {processed_files} m√∫sicas de {len(filenames)} at√© agora.")

                # Escrevendo no log apenas os nomes renomeados
                for name in formatted_names:
                    logging.info(f"Alterado para: {name}")
                    print(f"Alterado para: {name}")  # Exibindo no console
                print(f"‚úîÔ∏è {len(formatted_names)} m√∫sicas formatadas e registradas.")
            else:
                logging.error(f"‚ùå Erro ao formatar os nomes das m√∫sicas no chunk {i + 1}. Resposta vazia.")
                print(f"‚ùå Erro ao formatar os nomes das m√∫sicas.")
                return []

        except Exception as e:
            logging.error(f"‚ùå Erro ao formatar o nome da m√∫sica no chunk {i + 1}: {e}")
            print(f"‚ùå Erro ao formatar o nome da m√∫sica: {e}")
            return []

        # Adicionando um pequeno atraso entre as requisi√ß√µes para evitar bloqueios
        time.sleep(10)

    logging.info(f"Processamento completo. {len(formatted_names)} m√∫sicas formatadas.")
    print(f"Processamento completo. {len(formatted_names)} m√∫sicas formatadas.")  # Exibindo no console
    return formatted_names